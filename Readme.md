# Positional Embeddings in Transformer Models

This project demonstrates the implementation of absolute and rotary positional embeddings (RoPE) in a transformer model using PyTorch. The goal is to explore different positional embeddings mechanisms used to encode positions of tokens along with the context to improve the performance of self-attention mechanisms in transformers.

## Table of Contents

- [Overview](#overview)
- [Requirements](#requirements)
- [Installation](#installation)
- [References](#references)
